### Introduction to PCAThe goal of PCA is to reduce the **dimensionality** of multiple **correlated random variables** $\{x_i\}_{i=1}^n$ to a parsimonious set of **uncorrelated components**. Each such uncorrelated component is a linear combination of original random variables
$$\begin{align}y_i &= \mathbf{\gamma_i}^\top\,\mathbf{x}\\&= \sum_{j=1}^n\,\gamma_{ij}x_j\end{align}$$
The process to determine $\{\mathbf{\gamma}_i\}_{i=1}^m$ with $m\leq n$ is iterative.
Suppse the covariance matrix of $\{x_i\}_{i=1}^n$ is given by $\Sigma$. The procedure of PCA begins with determining the coefficient vector $\mathbf{\gamma}_1$ such that the variance of $y_1 = \sum_{j=1}^n\,\gamma_{1j}x_j$, i.e.$$\mathbf{\gamma}_1^\top\,\Sigma\,\mathbf{\gamma}_1$$is maximized. For the ensuing steps $i=2,\cdots$, the goal is to maximize variance $$\mathbf{\gamma}_i^\top\,\Sigma\,\mathbf{\gamma}_i$$when ensuring that $y_i$ is uncorrelated with all previous $y_j$ for $j=1,\cdots, i-1$.
The new component at $k^{th}$ step is called $k^{th}$ principal component ( **PC** ), the vector $\mathbf{\gamma}_k$ is defined as the vector of loadings for the $k^{th}$ principal component. The final goal is then to minimize the selected **PC** such that most of the **vairance** in $\mathbf{x}$ can be accounted for.
