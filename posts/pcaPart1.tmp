### Introduction to PCAThe goal of PCA is to reduce the **dimensionality** of multiple **correlated random variables** $\{x_i\}_{i=1}^n$ to a parsimonious set of **uncorrelated components**. Each such uncorrelated component is a linear combination of original random variables
$$\begin{align}y_i &= \mathbf{\gamma_i}^\top\,\mathbf{x}\\&= \sum_{j=1}^n\,\gamma_{ij}x_j\end{align}$$
The process to determine $\{\mathbf{\gamma}_i\}_{i=1}^m$ with $m\leq n$ is iterative.
Suppse the covariance matrix of $\{x_i\}_{i=1}^n$ is given by $\Sigma$. The procedure of PCA begins with determining the coefficient vector $\mathbf{\gamma}_1$ such that the variance of $y_1 = \sum_{j=1}^n\,\gamma_{1j}x_j$, i.e.$$\mathbf{\gamma}_1^\top\,\Sigma\,\mathbf{\gamma}_1$$is maximized. For the ensuing steps $i=2,\cdots$, the goal is to maximize variance $$\mathbf{\gamma}_i^\top\,\Sigma\,\mathbf{\gamma}_i$$when ensuring that $y_i$ is uncorrelated with all previous $y_j$ for $j=1,\cdots, i-1$.
The new component at $k^{th}$ step is called $k^{th}$ principal component ( **PC** ), the vector $\mathbf{\gamma}_k$ is defined as the vector of loadings for the $k^{th}$ principal component. The final goal is then to minimize the selected **PC** such that most of the **vairance** in $\mathbf{x}$ can be accounted for.
Consequently, the procedures of this optimizaation process lead to the equivalence of $\{\mathbf{\gamma}_i\}$ to the orthonormal eigenvectors of covariance matrix $\Sigma$, ordered **in descending way** by the size of corresponding eigenvalues.
Eventually we can write$$\mathbf{y} = \Gamma^\top\,\mathbf{x} = \left[ \mathbf{\gamma}_1^\top \mathbf{x} \cdots \mathbf{\gamma}_n^\top\mathbf{x} \right]$$where the columns of $\Gamma$ are the eigenvectors $\{\mathbf{\gamma}_i\}$.
Conversely, we note that $$\mathbf{x} = \Gamma \mathbf{y} = y_1\mathbf{\gamma}_1 + \cdots + y_n\mathbf{\gamma}_n$$indicating that the $n$-dimensional space can be spanned by the loading vectors $\{\mathbf{\gamma}_i\}$.
The eigenvalues $\lambda_k$ corresponding to each loading vector ( *eigenvector* ) $\mathbf{\gamma}_k$ are in fact the variance of PC $\mathbf{y}_k$. In financial markets, normally the first three eigenvalues can account for nearly $100\%$ of all variance, and the eigenvectors associated with them, $\mathbf{\gamma}_{1,2,3}$, are called the **shift, skew and curvature**.
It can be shown that a valid covariance or correlation matrix, which belongs to the class of **strictly positive or oscillatory** matrices, automatically has $j-1$ sign changes in the eigenvector corresponding to the $j^{th}$ largest eigenvalue. But the sign changes cannot provide sufficient information about the shape of eigenvector $\mathbf{\gamma}_{1,2,3}$
### Bisymmetric Matrices
**Definition 1**. Let $\mathbf{J} \in \mathbb{R}^{n\times n}$ be a matrix where only the anti-diagonal are all ones and everywhere else is zero.$$\mathbf{J} = \begin{pmatrix}0 & 0 & \cdots & \cdots & 0 & 1\\0 & 0 & \cdots & \cdots & 1 & 0\\\vdots & & \ddots & & & \vdots\\0 & 1 & \cdots & \cdots & 0 & 0\\1 & 0 & \cdots & \cdots & 0 & 0\end{pmatrix}$$A bisymmetric matrix $A \in \mathbb{R}^{n\times n}$ is symmetric w.r.t both diagonals, i.e.$$\mathbf{JAJ = A}.$$
**Definition 2**. A vector $\mathbf{\gamma}_s \in \mathbb{R}^n$ is called symmetric, if$$\mathbf{J\gamma}_s = \mathbf{\gamma}_s$$A vector $\mathbf{\gamma}_{ss} \in \mathbb{R}^n$ is called skew symmetric, if$$\mathbf{J\gamma}_{ss} = -\mathbf{\gamma}_{ss}$$
**Theorem 1**. Let $A \in \mathbb{R}^{n\times n}$ be bisymmetric matrix. For notation, let $\lceil x \rceil$ be the smallest integer $\geq x$, and $\lfloor x \rfloor$ be the largest integer $\leq x$. Then- If $n$ is even, matrix $A$ has $n/2$ skew symmetric eigenvectors and $n/2$ symmetric eigenvectors;- If $n$ is odd, matrix $A$ has $\lfloor n/2 \rfloor$ skew symmetric eigenvectors and $\lceil n/2 \rceil$ symmetric eigenvectors.Here without loss of generality we assume all eigenvectors are orthonormal.
Consider $3\times 3$ matrix $A$ of the below form$$A = \begin{pmatrix}a_1 & b & c\\b & a_2 & b\\c & b & a_1\end{pmatrix}$$
Based on the above thoerem, we know that for this type of matrix $A$, there is one skew symmetric orthonormal eigenvector, and two symmetric orthonormal eigenvectors.
**Theorem 2**. Matrix $A$ above has one skew symmetric, orthonormal eigenvector $\mathit{v}_{ss}$ and its corresponding eigenvalue $\lambda_{ss}$$$\mathit{v}_{ss} = \begin{pmatrix}\frac{1}{\sqrt{2}}\\0\\-\frac{1}{\sqrt{2}}\end{pmatrix} \qquad \lambda_{ss} = a_1 - c$$
Note that this eigenvector does not depend on the variables $a_{1,2}, b, c$. This form of vector is typical form of skew vector produced by a PCA for financial market data.
**Theorem 3**. Define$$d = \sqrt{(a_1 - a_2 + c)^2 + 8b^2}$$and$$\mathit{w}_{s1} = \begin{pmatrix}\frac{a_1-a_2+c+d}{4b}\\1\\\frac{a_1-a_2+c+d}{4b}\end{pmatrix} \qquad \mathit{w}_{s2} = \begin{pmatrix}\frac{a_1-a_2+c-d}{4b}\\1\\\frac{a_1-a_2+c-d}{4b}\end{pmatrix}$$Matrix $A$ above hence has symmetric, orthonormal eigenvectors $$\mathit{v}_{s1} = \frac{\mathit{w}_{s1}}{\|\mathit{w}_{s1}\|}, \qquad \mathit{v}_{s2} = \frac{\mathit{w}_{s2}}{\|\mathit{w}_{s2}\|}$$with corresponding eigenvalues$$\lambda_{s1} = \frac{a_1+a_2+c+d}{2}, \qquad \lambda_{s2}=\frac{a_1+a_2+c-d}{2}$$where $\|\|$ is the Euclidean norm.
Symmetric vectors are typical representatives of level and curvature effects.
Consider this type of matrix only. The question now is can PCA recover the ``true'' economic effects?
### Potential Interpretation Problems for PCA
Consider three random variables defined as follows$$\begin{align}&\sigma_+ = a + b_+\\&\sigma_0 = a + b_0\\&\sigma_- = a + b_-\end{align}$$where $b_+, b_0, b_-, a$ are **independent** random variables ( *only independency is assumed* ). Assuming that $b_-, b_+$ have the same variance $\mathrm{V}(b_-) = \mathrm{V}(b_+)$, the covariance matrix $Sigma$ will be determined by$$\begin{align}&\mathrm{Cov}(\sigma_-, \sigma_-) = \mathrm{V}(a) + \mathrm{V}(b_-), \quad \mathrm{Cov}(\sigma_-, \sigma_0) = \mathrm{V}(a), \quad \mathrm{Cov}(\sigma_-, \sigma_+) = \mathrm{V}(a)\\&\mathrm{Cov}(\sigma_0, \sigma_0) = \mathrm{V}(a) + \mathrm{V}(b_0), \quad \mathrm{Cov}(\sigma_0, \sigma_+) = \mathrm{V}(a)\\&\mathrm{Cov}(\sigma_+, \sigma_+) = \mathrm{V}(a) + \mathrm{V}(b_+)\end{align}$$In other words,$$\Sigma = \begin{pmatrix}\mathrm{V}(a)+\mathrm{V}(b_+) & \mathrm{V}(a) & \mathrm{V}(a)\\\mathrm{V}(a) & \mathrm{V}(a)+ \mathrm{V}(a) & \mathrm{V}(b_0)\\\mathrm{V}(a) & \mathrm{V}(a) & \mathrm{V}(a)+\mathrm{V}(b_-)\end{pmatrix}$$
The covariance matrix $\Sigma$ is bisymmetric of order $3$. By results we have in last section, it has a skew symmetric eigenvector ( factor loading vector ) defined independent of matrix variables, and a corresponding eigenvalue $\mathrm{V}(\sigma_+) - \mathrm{V}(a) = \mathrm{V}(b_+)$, which is the **variance** of principla component of this eigenvector. 
However, since the system is comprised of $3$ independent random variables, it does not intuitively suggest the presence of a skew vector. While PCA should indicate presence of level factor, generated by variable $a$, there is no economic justification for the additional curvature and skew components. The skew and curvature solely are consequences of the bisymmetry of covariance matrix.
Next we consider such a case where all three random variables $\sigma_{+,-,0}$ are driven by$$\begin{align}& a \sim \mathrm{N}(0, 0.2)\\& b_- \sim \mathrm{N}(0, 0.25)\\& b_0 \sim \mathrm{U}[0, 1]\\& b_+ \sim \mathrm{Exp}(2)\end{align}$$$50000$ realizations of these random variables are generated independently. 

import numpy as npN = 50000A = np.random.normal( 0, np.sqrt( 0.2 ), N )bminus = np.random.normal( 0, np.sqrt( 0.25 ), N )bzero = np.random.uniform( 0, 1, N )bplus = np.random.exponential( 1./2, N )sigmaplus = A + bplussigmazero = A + bzerosigmaminus = A + bminusx = np.array( [ sigmaplus, sigmazero, sigmaminus ] )S = np.cov( x )print Sevalue, evec = np.linalg.eig( S )for k, v in enumerate( evalue ):    print v, evec[:, k ]

Now we can clearly see that the covariance matrix is close to bisymmetry. Also we can observe that the covariance matrix, as expected, has one skew symmetric eigenvector and two symmetric eigenvectors. We see that the skew symmetric eigenvector closely matches the theoretical vaue
The above experiment clearly demonstrates that even without underlying skew effect, or economic reasons, purely random processes can produce skew and curvatures.
